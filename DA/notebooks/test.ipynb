{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e1e0618-614a-4fc1-a146-0bec4c47c1b1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'cp949' codec can't decode byte 0xeb in position 73: illegal multibyte sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Embedding, LSTM, Dense\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# 데이터셋 불러오기\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../data/raw.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcp949\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 예: 'data.csv'\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# 텍스트 데이터 전처리\u001b[39;00m\n\u001b[0;32m     13\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m Tokenizer()\n",
      "File \u001b[1;32mD:\\ShipSupplies\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    900\u001b[0m     dialect,\n\u001b[0;32m    901\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    909\u001b[0m )\n\u001b[0;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\ShipSupplies\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mD:\\ShipSupplies\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\ShipSupplies\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1679\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1676\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m   1678\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1679\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mapping[engine](f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions)\n\u001b[0;32m   1680\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1681\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mD:\\ShipSupplies\\venv\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:93\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype_backend\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;66;03m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[0;32m     92\u001b[0m     import_optional_dependency(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader \u001b[38;5;241m=\u001b[39m parsers\u001b[38;5;241m.\u001b[39mTextReader(src, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munnamed_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39munnamed_cols\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "File \u001b[1;32mD:\\ShipSupplies\\venv\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:548\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mD:\\ShipSupplies\\venv\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:637\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mD:\\ShipSupplies\\venv\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:848\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mD:\\ShipSupplies\\venv\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:859\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mD:\\ShipSupplies\\venv\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:2017\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'cp949' codec can't decode byte 0xeb in position 73: illegal multibyte sequence"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# 데이터셋 불러오기\n",
    "dataset = pd.read_csv('../data/raw.csv', encoding='cp949')  # 예: 'data.csv'\n",
    "\n",
    "# 텍스트 데이터 전처리\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(dataset['Machinery'])\n",
    "sequences = tokenizer.texts_to_sequences(dataset['Machinery'])\n",
    "word_index = tokenizer.word_index\n",
    "max_sequence_length = max(len(seq) for seq in sequences)\n",
    "X = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# 카테고리 데이터 전처리\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(dataset['key2'])\n",
    "\n",
    "# 데이터셋을 학습용과 테스트용으로 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 딥러닝 모델 생성\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1, 100, input_length=max_sequence_length))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(len(label_encoder.classes_), activation='softmax'))\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# 모델 학습\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# 새로운 데이터 예측\n",
    "new_text = ['NO.2 GENERATOR ENGINE']\n",
    "new_sequence = tokenizer.texts_to_sequences(new_text)\n",
    "new_X = pad_sequences(new_sequence, maxlen=max_sequence_length)\n",
    "predictions = model.predict(new_X)\n",
    "predicted_class = label_encoder.inverse_transform(predictions.argmax(axis=-1))\n",
    "print(\"새로운 데이터 예측 카테고리:\", predicted_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab832b08-444d-4802-8745-8f5978bb2ce6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90650796-8bb3-4edf-aacc-1f09e0644746",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "041803ca-8f5e-453c-8ca8-ff3947aecdd9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\nSmooth like butter', '\\nLike a criminal undercover', \"\\nGon' pop like trouble\", \"\\nBreakin' into your heart like that\", '\\nCool shade stunner', '\\nYeah, I owe it all to my mother', '\\nHot like summer', \"\\nYeah, I'm makin' you sweat like that\", '\\nBreak it down', '\\n']\n",
      "[['Smooth', 'like', 'butter'], ['Like', 'a', 'criminal', 'undercover'], [\"Gon'\", 'pop', 'like', 'trouble'], [\"Breakin'\", 'into', 'your', 'heart', 'like', 'that'], ['Cool', 'shade', 'stunner'], ['Yeah,', 'I', 'owe', 'it', 'all', 'to', 'my', 'mother'], ['Hot', 'like', 'summer'], ['Yeah,', \"I'm\", \"makin'\", 'you', 'sweat', 'like', 'that'], ['Break', 'it', 'down'], []]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['Smooth', 'like', 'butter'],\n",
       " ['Like', 'a', 'criminal', 'undercover'],\n",
       " [\"Gon'\", 'pop', 'like', 'trouble'],\n",
       " [\"Breakin'\", 'into', 'your', 'heart', 'like', 'that'],\n",
       " ['Cool', 'shade', 'stunner'],\n",
       " ['Yeah,', 'I', 'owe', 'it', 'all', 'to', 'my', 'mother'],\n",
       " ['Hot', 'like', 'summer'],\n",
       " ['Yeah,', \"I'm\", \"makin'\", 'you', 'sweat', 'like', 'that'],\n",
       " ['Break', 'it', 'down'],\n",
       " []]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# BTS 의 butter 가사\n",
    "\n",
    "butter = \"\"\"\n",
    "Smooth like butter/\n",
    "Like a criminal undercover/\n",
    "Gon' pop like trouble/\n",
    "Breakin' into your heart like that/\n",
    "Cool shade stunner/\n",
    "Yeah, I owe it all to my mother/\n",
    "Hot like summer/\n",
    "Yeah, I'm makin' you sweat like that/\n",
    "Break it down/\n",
    "\"\"\"\n",
    "print(butter.split(\"/\"))\n",
    "butter = butter.split(\"/\")\n",
    "# butter = [line.strip() for line in butter]\n",
    "butter = [line.split() for line in butter]\n",
    "\n",
    "print(butter)\n",
    "butter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e3a9f4b-a84d-4f02-8879-ef77a90ea48e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer1.index_docs defaultdict(<class 'int'>, {1: 6, 6: 1, 5: 1, 9: 1, 8: 1, 7: 1, 12: 1, 11: 1, 10: 1, 14: 1, 2: 2, 13: 1, 15: 1, 16: 1, 19: 1, 18: 1, 17: 1, 25: 1, 4: 2, 22: 1, 21: 1, 24: 1, 3: 2, 20: 1, 23: 1, 27: 1, 26: 1, 30: 1, 29: 1, 28: 1, 31: 1, 32: 1, 33: 1})\n",
      "tokenizer1.index_word {1: 'like', 2: 'that', 3: 'yeah,', 4: 'it', 5: 'smooth', 6: 'butter', 7: 'a', 8: 'criminal', 9: 'undercover', 10: \"gon'\", 11: 'pop', 12: 'trouble', 13: \"breakin'\", 14: 'into', 15: 'your', 16: 'heart', 17: 'cool', 18: 'shade', 19: 'stunner', 20: 'i', 21: 'owe', 22: 'all', 23: 'to', 24: 'my', 25: 'mother', 26: 'hot', 27: 'summer', 28: \"i'm\", 29: \"makin'\", 30: 'you', 31: 'sweat', 32: 'break', 33: 'down'}\n"
     ]
    }
   ],
   "source": [
    "# 학습에 사용할 단어장을 만들어 보자!\n",
    "tokenizer1 = tf.keras.preprocessing.text.Tokenizer()\n",
    "\n",
    "# Updates internal vocabulary based on a list of sequences.\n",
    "tokenizer1.fit_on_texts(butter)\n",
    "# print('tokenizer1.word_index', tokenizer1.word_index)\n",
    "\n",
    "print('tokenizer1.index_docs', tokenizer1.index_docs)\n",
    "print('tokenizer1.index_word', tokenizer1.index_word) # 빈도수 순으로 순서 보장\n",
    "# print('tokenizer1.word_index', tokenizer1.word_index) #\n",
    "# print('tokenizer1.word_docs', tokenizer1.word_docs)\n",
    "# print('tokenizer1.word_counts', tokenizer1.word_counts)\n",
    "# print('tokenizer1.word_counts', sorted(tokenizer1.word_counts.items(), key=lambda x: x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32a3951b-40cf-4851-8c11-7e23b73634dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5, 1, 6],\n",
       " [1, 7, 8, 9],\n",
       " [10, 11, 1, 12],\n",
       " [13, 14, 15, 16, 1, 2],\n",
       " [17, 18, 19],\n",
       " [3, 20, 21, 4, 22, 23, 24, 25],\n",
       " [26, 1, 27],\n",
       " [3, 28, 29, 30, 31, 1, 2],\n",
       " [32, 4, 33],\n",
       " []]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어장을 이용해 butter 가사를 숫자로 변경해 보자!\n",
    "encoded_butter=tokenizer1.texts_to_sequences(butter) # 텍스트를 숫자(인덱스) 로 변경해줌\n",
    "encoded_butter\n",
    "# tokenizer1.texts_to_sequences(butter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515aba2d-409c-4a62-935d-5195064e7e25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab54009-20b5-4f10-aabe-996a35559c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OOV: Out-Of-Vocabulary\n",
    "# 사전에서 빈도수 높은 단어 5개만 사용\n",
    "# 사전에 없는 단어는? <OOV> 처리!\n",
    "tokenizer1 = tf.keras.preprocessing.text.Tokenizer(num_words=5, oov_token='<OOV>')\n",
    "tokenizer1.fit_on_texts(butter)\n",
    "print('tokenizer1.index_docs', tokenizer1.index_docs)\n",
    "print('tokenizer1.index_word', tokenizer1.index_word)\n",
    "print('tokenizer1.word_index', tokenizer1.word_index)\n",
    "encoded_butter = tokenizer1.texts_to_sequences(butter)\n",
    "encoded_butter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accd8295-2a2c-473f-b128-b9dc2ef0b5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장의 길이가 다르므로 일치시켜 보자! \n",
    "# (MNIST 이미지 데이터의 모양은 모두 28 x 28)\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "encoded_butter=pad_sequences(encoded_butter)\n",
    "pad_sequences(encoded_butter)\n",
    "#pad_sequences(encoded_butter, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2adf15a-78c0-4bd4-8a73-713d7d6ad73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_sequences(encoded_butter, maxlen=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca23ab1-65ec-4e1f-9760-316e9fac5f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense Layer 에 입력으로 사용해 보자!\n",
    "# one-hot encoding\n",
    "# Converts a class vector (integers) to binary class matrix.\n",
    "# tf.keras.utils.to_categorical([0, 1, 2, 3], num_classes=4)\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "to_categorical(encoded_butter[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6f148f-ea36-427c-b7e4-e8da63ac613c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a406cdf9-e3a5-4d01-b301-e3bb37247569",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3572b2-6cfd-4cee-b2eb-288abb26ab16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fe0692-54fd-4b35-8a98-f88c2128bd7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c95a6c-c3a9-41dc-a300-8a8e74dbb552",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be8a8c4-3564-44d8-99b7-20a21415efac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
